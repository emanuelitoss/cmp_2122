{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bilby.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyONDiqZcnQo7bRSONMUm5fB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"quA5zQqJ3DkE"},"source":["<img style=\"float: left;padding: 1.3em\" src=\"https://indico.in2p3.fr/event/18313/logo-786578160.png\">  \n","\n","This notebook puts together tutorials available at the [Gravitational-Wave Open Science Center (GWOSC) website](https://www.gw-openscience.org)\n","\n","Topics:\n","\n","* Plotting and manipulating publicly available gravitational-wave posterior samples. \n","* Carrying out parameter estimation on open gravitational-wave data.\n","\n","One of the modules we will be using in this notebook is [Bilby](https://lscsoft.docs.ligo.org/bilby/).  Bilby is a user-friendly Bayesian inference library primarily designed for inference of compact binary coalescence events in interferometric data."]},{"cell_type":"markdown","source":["# Part 3.1:  Parameter estimation for compact object mergers -- Using and interpreting posterior samples\n","\n","This is a simple demonstration on loading and viewing public Bayesian inference results pertaining to gravitational-wave signals.\n","\n","The data used here is downloaded from the public DCC page [LIGO-P1800370](https://dcc.ligo.org/LIGO-P1800370/public)."],"metadata":{"id":"cBykO3SfDK2s"}},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"taEFmvys3DkG"},"source":["## Installation\n","\n","To deal with time series we use [GWPy](https://gwpy.github.io)'s `TimeSeries`.\n","\n","To generate waveforms to compare to the data, we need [LALSuite](https://lscsoft.docs.ligo.org/lalsuite/)."]},{"cell_type":"code","execution_count":1,"metadata":{"Collapsed":"false","id":"Z2fAXT1v3DkH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640020174401,"user_tz":-60,"elapsed":41033,"user":{"displayName":"Francesco Pannarale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04296007763837408065"}},"outputId":"778033d7-909f-41c2-bc8f-653e736ee2b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 10.2 MB 6.5 MB/s \n","\u001b[?25h  Building wheel for corner (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 27.3 MB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 11.6 MB 18.7 MB/s \n","\u001b[K     |████████████████████████████████| 1.4 MB 52.5 MB/s \n","\u001b[K     |████████████████████████████████| 1.6 MB 48.6 MB/s \n","\u001b[K     |████████████████████████████████| 51 kB 8.1 MB/s \n","\u001b[K     |████████████████████████████████| 87 kB 8.5 MB/s \n","\u001b[K     |████████████████████████████████| 45 kB 4.2 MB/s \n","\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s \n","\u001b[K     |████████████████████████████████| 3.6 MB 27.3 MB/s \n","\u001b[?25h  Building wheel for bilby (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for ligo-segments (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for lscsoft-glue (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# -- Use the following line for google colab\n","! pip install -q 'corner==2.0.1' 'astropy==4.0.3'\n","! pip install -q 'lalsuite==6.82' 'bilby==1.0.4' 'gwpy==2.0.2' #2.0.2"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"qrBuraTe3DkK"},"source":["**Important**: With Google Colab, you may need to restart the runtime after running the cell above."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"6uLsysdP3DkL"},"source":["## Initialization"]},{"cell_type":"code","execution_count":2,"metadata":{"Collapsed":"false","id":"DiR88NzK3DkM","executionInfo":{"status":"ok","timestamp":1640020175623,"user_tz":-60,"elapsed":1227,"user":{"displayName":"Francesco Pannarale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04296007763837408065"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import pandas as pd\n","import corner\n","import bilby"]},{"cell_type":"markdown","metadata":{"id":"9TRFbImzCRP0"},"source":["We check the Bilby version"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UrGN5ZLqCRP0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640020175623,"user_tz":-60,"elapsed":21,"user":{"displayName":"Francesco Pannarale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04296007763837408065"}},"outputId":"1afe8622-8e2b-4802-8d4c-d1dd92786edb"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.0.4: release\n"]}],"source":["print(bilby.__version__)"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"iBZpF7363DkO"},"source":["## Get the data\n","\n","Select the GW150914 event."]},{"cell_type":"code","execution_count":4,"metadata":{"Collapsed":"false","id":"sPBQg9cA3DkO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640020176373,"user_tz":-60,"elapsed":767,"user":{"displayName":"Francesco Pannarale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04296007763837408065"}},"outputId":"87641a56-057a-4b3b-960e-5179d49ee67a"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-12-20 17:09:35--  https://dcc.ligo.org/LIGO-P1800370/public/GW150914_GWTC-1.hdf5\n","Resolving dcc.ligo.org (dcc.ligo.org)... 131.215.125.144\n","Connecting to dcc.ligo.org (dcc.ligo.org)|131.215.125.144|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://dcc.ligo.org/public/0157/P1800370/005/GW150914_GWTC-1.hdf5 [following]\n","--2021-12-20 17:09:35--  https://dcc.ligo.org/public/0157/P1800370/005/GW150914_GWTC-1.hdf5\n","Reusing existing connection to dcc.ligo.org:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7026464 (6.7M)\n","Saving to: ‘GW150914_GWTC-1.hdf5’\n","\n","GW150914_GWTC-1.hdf 100%[===================>]   6.70M  --.-KB/s    in 0.1s    \n","\n","2021-12-20 17:09:36 (57.0 MB/s) - ‘GW150914_GWTC-1.hdf5’ saved [7026464/7026464]\n","\n"]}],"source":["label = 'GW150914'\n","filename = label+'_GWTC-1.hdf5'\n","\n","# If you do not have wget installed, simply manually download \n","# https://dcc.ligo.org/LIGO-P1800370/public/GW150914_GWTC-1.hdf5 \n","# from your browser\n","! wget https://dcc.ligo.org/LIGO-P1800370/public/{filename}"]},{"cell_type":"code","execution_count":5,"metadata":{"Collapsed":"false","id":"LlIkUSXc3DkR","executionInfo":{"status":"ok","timestamp":1640020176374,"user_tz":-60,"elapsed":5,"user":{"displayName":"Francesco Pannarale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04296007763837408065"}}},"outputs":[],"source":["posterior = h5py.File('./'+filename, 'r')"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"AyfL5UYJ3DkU"},"source":["### Looking into the file structure\n","\n","[Hdf5](https://www.hdfgroup.org/solutions/hdf5/) (hierarchical data format) files work a bit like dictionaries."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"tgzznwnT3DkV"},"outputs":[],"source":["print('This file contains four datasets: ', posterior.keys())"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"bObo647S3DkY"},"source":["This data file contains several datasets.\n","* Two datasets use separate models (approximants) for the gravitational waveform, namely `IMRPhenomPv2` and `SEOBNRv3`.  [See the [paper](https://dcc.ligo.org/LIGO-P1800307) for more details.]\n","* There is a joint dataset that combines equal numbers of samples from each individual model.\n","* Finally, there is a dataset containing samples drawn from the prior used for the analyses."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"D12e3TRs3DkZ"},"outputs":[],"source":["print(posterior['Overall_posterior'].dtype.names)"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"bM94W-lv3Dkb"},"source":["Here are some brief descriptions of these parameters and their uses:\n","\n"," * `luminosity_distance_Mpc`: luminosity distance [Mpc]\n","\n"," * `m1_detector_frame_Msun`: primary (larger) black hole mass (detector frame) [solar mass]\n","\n"," * `m2_detector_frame_Msun`: secondary (smaller) black hole mass (detector frame) [solar mass]\n","\n"," * `right_ascension`, `declination`: right ascension and declination of the source [rad].\n","\n"," * `costheta_jn`: cosine of the angle between line of sight and total angular momentum vector of the source.\n","\n"," * `spin1`, `costilt1`: primary (larger) black hole spin magnitude (dimensionless) and cosine of the zenith angle between the spin and the orbital angular momentum vector of the source.\n","\n"," * `spin2`, `costilt2`: secondary (smaller) black hole spin magnitude (dimensionless) and cosine of the zenith angle between the spin and the orbital angular momentum vector of the source.\n","\n","A convenient (and pretty) way to load up an array of samples is to feed it as a NumPy array to [pandas](https://pandas.pydata.org/):"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"su0I7kOC3Dkc"},"outputs":[],"source":["samples = pd.DataFrame.from_records(np.array(posterior['Overall_posterior']))"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"j_FO94bB3Dke"},"outputs":[],"source":["samples"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"cIAbUy1q3Dkh"},"source":["These are all the samples stored in the `Overall` dataset. "]},{"cell_type":"markdown","source":["## Plotting\n","\n","We can plot all of them with, for instance, the usual [corner](https://corner.readthedocs.io/en/latest/) package:"],"metadata":{"id":"sMV5bGPJFvCK"}},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"NvG83vQo3Dkh"},"outputs":[],"source":["corner.corner(samples,labels=['costhetajn',\n","                              'distance [Mpc]',\n","                              'ra',\n","                              'dec',\n","                              'mass1 [Msun]',\n","                              'mass2 [Msun]',\n","                              'spin1',\n","                              'spin2',\n","                              'costilt1',\n","                              'costilt2']);"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"GglaDuCk3Dkk"},"source":["We can manualy select one parameter and plot the marginalised distributions from the four distinct data sets.  We do this for the `luminosity distance`:"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"0ao_u80H3Dkl"},"outputs":[],"source":["for label in ['prior', 'IMRPhenomPv2_posterior', 'SEOBNRv3_posterior', 'Overall_posterior']:\n","    plt.hist(posterior[label]['luminosity_distance_Mpc'], bins = 100, label=label.replace('_posterior',''), alpha=0.8, density=True)\n","\n","plt.xlabel(r'$D_L (Mpc)$')\n","plt.ylabel('Probability Density Function')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"6u5Ba3v53Dkr"},"source":["### Computing new quantities\n","\n","The masses given are the ones measured at the detector, i.e., in the *detector frame*. To determine the actual (*source frame*) masses of the source black holes, we need to correct for the cosmological redshift of the gravitational wave. This forces us to assume a cosmology.\n","\n","And, of course, we can inform Python about cosmology 😀"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"hG06jTn53Dkr"},"outputs":[],"source":["import astropy.units as u\n","from astropy.cosmology import Planck15, z_at_value"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"lv1EoAny3Dku"},"source":["We compute the redshift value for all the samples (using only their distance value). See [astropy.cosmology](http://docs.astropy.org/en/stable/api/astropy.cosmology.z_at_value.html) for implementation details, in particular how to make the following more efficient:"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"RS7AD2dX3Dkv"},"outputs":[],"source":["z = np.array([z_at_value(Planck15.luminosity_distance, dist * u.Mpc) for dist in samples['luminosity_distance_Mpc']])"]},{"cell_type":"markdown","source":["We add new entries to `samples`."],"metadata":{"id":"aixe_d5uJpHK"}},{"cell_type":"code","source":["print(samples.keys())\n","print(samples.shape)"],"metadata":{"id":"D5uiD6SpJu6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"29S-5y-k3Dkx"},"outputs":[],"source":["samples['m1_source_frame_Msun'] = samples['m1_detector_frame_Msun']/(1.0+z)\n","samples['m2_source_frame_Msun'] = samples['m2_detector_frame_Msun']/(1.0+z)\n","samples['redshift'] = z"]},{"cell_type":"code","source":["print(samples.keys())\n","print(samples.shape)"],"metadata":{"id":"EgHEg6JhKP39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"lNM077Db3Dk0"},"source":["And we can plot the marginalised probability density functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"QwFJOBJ73Dk1"},"outputs":[],"source":["corner.corner(samples[['m1_source_frame_Msun',\n","                       'm2_source_frame_Msun',\n","                       'redshift']],\n","              labels=['m1 (source)',\n","                      'm2 (source)',\n","                      'z']);"]},{"cell_type":"markdown","metadata":{"id":"bRfb56q_CQ9Y"},"source":["## Calculating credible intervals\n","Let's see how we can use the [Bilby package](https://lscsoft.docs.ligo.org/bilby/) to calcuate summary statistics for the posterior, e.g., the median and 90% credible interval.\n","\n","The [chirp mass](https://en.wikipedia.org/wiki/Chirp_mass) is an important quantity in GW physics.  It is the main driver of the GW phase.  Its definition is\n","\n","$$\n","\\mathcal{M} = \\frac{(m_1m_2)^{3/5}}{(m_1+m_2)^{1/5}}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVlt0mPECQ9b"},"outputs":[],"source":["# Calculate the detector frame chirp mass\n","mchirp = ((samples['m1_detector_frame_Msun'] * samples['m2_detector_frame_Msun'])**(0.6))/\\\n","         (samples['m1_detector_frame_Msun'] + samples['m2_detector_frame_Msun'])**(0.2)\n","\n","# Initialize a SampleSummary object to describe the chirp mass posterior samples\n","chirp_mass_samples_summary = bilby.core.utils.SamplesSummary(samples=mchirp, average='median')\n","\n","# Output the desired information\n","print('Median chirp mass: {:.1f} Msun'.format(chirp_mass_samples_summary.median))\n","print('90% credible interval for the chirp mass: [{:.1f}, {:.1f}] Msun'.format(chirp_mass_samples_summary.lower_absolute_credible_interval,\n","                                                                        chirp_mass_samples_summary.upper_absolute_credible_interval))"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"OjUNeFsxyguu"},"source":["# Part 3.2: Parameter estimation on GW150914 using open data\n","\n","This example estimates the non-spinning parameters of the binary black hole system using commonly used prior distributions.\n","   \n","Find more examples at https://lscsoft.docs.ligo.org/bilby/examples.html"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"XK8fHu13ygu1"},"source":["## Initialization\n","\n","We begin by importing some commonly used functions"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"HyRSGt6cygu2"},"outputs":[],"source":["from bilby.core.prior import Uniform\n","from bilby.gw.conversion import convert_to_lal_binary_black_hole_parameters, generate_all_bbh_parameters\n","\n","from gwpy.timeseries import TimeSeries"]},{"cell_type":"markdown","metadata":{"id":"e9v5DKY9CRP1"},"source":["### Set up empty interferometers\n","\n","We will be using data from the Hanford (H1) and Livinston (L1) ground-based gravitational wave detectors. To start, we create two \"empty\" interferometers. These are empty in the sense that they do not have any strain data. But, they know about the orientation and location of their respective namesakes. It may also be interesting to note that they are initialised with the planned design sensitivity power spectral density of advanced LIGO - we will overwrite this later on, but it is often useful for simulations, as we saw in a previous notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j16JaGzDCRP1"},"outputs":[],"source":["H1 = bilby.gw.detector.get_empty_interferometer(\"H1\")\n","L1 = bilby.gw.detector.get_empty_interferometer(\"L1\")"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"_Hd4d4KVygu6"},"source":["## Getting the data: GW150914\n","\n","Once more, we pick GW150914 to demonstrate things.\n","\n","Our first task is to obtain the relevant data.  To do so, we need to know the trigger time: we already know how to do this with `gwosc`.  [Alternatively we can look it up manually on the [GWOSC page](https://www.gw-openscience.org/events/GW150914/).]"]},{"cell_type":"code","source":["# -- Uncomment following line if running in Google Colab\n","! pip install -q 'gwosc==0.5.4'"],"metadata":{"id":"1QxE0Qf0OrG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gwosc.datasets import event_gps\n","time_of_event = event_gps('GW150914')\n","print(time_of_event)"],"metadata":{"id":"rfqY07aEOQuU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U01-XRwaCRP2"},"source":["We use [GWPy](https://gwpy.github.io/) to download the open strain data.\n","\n","To analyse GW150914, we will use a 4s period duration centered on the event itself. It is standard to choose the data such that it always includes a \"post trigger duration\" of 2s. That is, there are always 2s of data after the trigger time. We therefore define all times relative to the trigger time, duration and this post-trigger duration."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"szXw1RXECRP2"},"outputs":[],"source":["# Definite times in relation to the trigger time (time_of_event), duration and post_trigger_duration\n","post_trigger_duration = 2\n","duration = 4\n","analysis_start = time_of_event + post_trigger_duration - duration\n","analysis_end = analysis_start + duration\n","\n","# Fetch the open data\n","H1_analysis_data = TimeSeries.fetch_open_data(\n","    \"H1\", analysis_start, analysis_end, sample_rate=4096, cache=True)\n","\n","L1_analysis_data = TimeSeries.fetch_open_data(\n","    \"L1\", analysis_start, analysis_end, sample_rate=4096, cache=True)"]},{"cell_type":"markdown","metadata":{"id":"HJxv8g87CRP3"},"source":["Here, `H1_analysis_data` and its L1 counterpart are GWPy `TimeSeries` objects. Remember that as such we can readily plot the data itself:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLfZ4fdsCRP3"},"outputs":[],"source":["H1_analysis_data.plot()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6L3nC_PLCRP3"},"source":["## Initialise the Bilby interferometers with the strain data\n","\n","Now, we pass the downloaded strain data to our `H1` and `L1` Bilby interferometer objects. For other methods to set the strain data, see the various `set_strain_data*` methods."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LS7QgGccCRP4"},"outputs":[],"source":["H1.set_strain_data_from_gwpy_timeseries(H1_analysis_data)\n","L1.set_strain_data_from_gwpy_timeseries(L1_analysis_data)"]},{"cell_type":"markdown","metadata":{"id":"fOmbFZ4VCRP4"},"source":["### Download the power spectral data\n","\n","Parameter estimation relies on having a power spectral density (PSD) - an estimate of the coloured noise properties of the data. Here, we will create a PSD using off-source data. For a review of methods to estimate PSDs, see, e.g. [Chatziioannou et al. (2019)](https://ui.adsabs.harvard.edu/abs/2019PhRvD.100j4004C/abstract).\n","\n","Again, we need to download this from the open strain data. We start by figuring out the amount of data needed. In this case: 32 times the analysis duration. We fetch the segment with this duration immediately preceding the analysis segment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20Rq1cGhCRP4"},"outputs":[],"source":["psd_duration = duration * 32\n","psd_start_time = analysis_start - psd_duration\n","psd_end_time = psd_start_time + psd_duration\n","\n","H1_psd_data = TimeSeries.fetch_open_data(\n","    \"H1\", psd_start_time, psd_end_time, sample_rate=4096, cache=True)\n","\n","L1_psd_data = TimeSeries.fetch_open_data(\n","    \"L1\", psd_start_time, psd_end_time, sample_rate=4096, cache=True)"]},{"cell_type":"markdown","metadata":{"id":"0_MnYelECRP4"},"source":["Having obtained the data to generate the PSD, we now use the standard [gwpy psd](https://gwpy.github.io/docs/stable/api/gwpy.timeseries.TimeSeries.html#gwpy.timeseries.TimeSeries.psd) method to calculate the PSD. Here, the `psd_alpha` variable is converting the `roll_off` applied to the strain data into the fractional value used by `gwpy`. This applies a window with an appropriate shape to the time-domain data."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"xcrd9z2MCRP5"},"outputs":[],"source":["psd_alpha = 2 * H1.strain_data.roll_off / duration\n","H1_psd = H1_psd_data.psd(fftlength=duration, overlap=0, window=(\"tukey\", psd_alpha), method=\"median\")\n","L1_psd = L1_psd_data.psd(fftlength=duration, overlap=0, window=(\"tukey\", psd_alpha), method=\"median\")"]},{"cell_type":"markdown","metadata":{"id":"wwitvloQCRP5"},"source":["### Initialise the PSD\n","Now that we have PSDs for H1 and L1, we can overwrite the `power_spectal_density` attribute of our interferometers with a new PSD."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnNbc6_pCRP5"},"outputs":[],"source":["H1.power_spectral_density = bilby.gw.detector.PowerSpectralDensity(\n","    frequency_array=H1_psd.frequencies.value, psd_array=H1_psd.value)\n","L1.power_spectral_density = bilby.gw.detector.PowerSpectralDensity(\n","    frequency_array=L1_psd.frequencies.value, psd_array=L1_psd.value)"]},{"cell_type":"markdown","metadata":{"id":"GRLrJBAsCRP5"},"source":["### Looking at the data\n","Okay, we have spent a bit of time now downloading and initializing things. Let's check that everything makes sense. To do this, we'll plot our analysis data alongside the amplitude spectral density (ASD); this is just the square root of the PSD and has the right units to be comparable to the frequency-domain strain data."]},{"cell_type":"code","source":["fig, ax = plt.subplots()\n","idxs = H1.strain_data.frequency_mask  # This is a boolean mask of the frequencies which we will use in the analysis\n","ax.loglog(H1.strain_data.frequency_array[idxs],\n","          np.abs(H1.strain_data.frequency_domain_strain[idxs]))\n","ax.loglog(H1.power_spectral_density.frequency_array[idxs],\n","          H1.power_spectral_density.asd_array[idxs])\n","ax.set_xlabel(\"Frequency [Hz]\")\n","ax.set_ylabel(\"Strain [strain/$\\sqrt{Hz}$]\")\n","plt.show()"],"metadata":{"id":"o-1VJ-hyPY-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qpseKkZCRP5"},"outputs":[],"source":["fig, ax = plt.subplots()\n","idxs = L1.strain_data.frequency_mask  # This is a boolean mask of the frequencies which we will use in the analysis\n","ax.loglog(L1.strain_data.frequency_array[idxs],\n","          np.abs(L1.strain_data.frequency_domain_strain[idxs]))\n","ax.loglog(L1.power_spectral_density.frequency_array[idxs],\n","          L1.power_spectral_density.asd_array[idxs])\n","ax.set_xlabel(\"Frequency [Hz]\")\n","ax.set_ylabel(\"Strain [strain/$\\sqrt{Hz}$]\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8xwQH1xdCRP5"},"source":["What is happening at high frequencies? This is an artifact of the downsampling applied to the data.  Note that we downloaded the 4096Hz data which is downsampled from 16384Hz. We are not really interested in the data at these high frequencies so let's adjust the maximum frequency used in the analysis to 1024 Hz and plot things again."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCkHdbJRCRP6"},"outputs":[],"source":["H1.maximum_frequency = 1024\n","L1.maximum_frequency = 1024"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OMX8QEVqCRP6"},"outputs":[],"source":["fig, ax = plt.subplots()\n","idxs = H1.strain_data.frequency_mask\n","ax.loglog(H1.strain_data.frequency_array[idxs],\n","          np.abs(H1.strain_data.frequency_domain_strain[idxs]))\n","ax.loglog(H1.power_spectral_density.frequency_array[idxs],\n","          H1.power_spectral_density.asd_array[idxs])\n","ax.set_xlabel(\"Frequency [Hz]\")\n","ax.set_ylabel(\"Strain [strain/$\\sqrt{Hz}$]\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xoZMUwHyCRP6"},"source":["Okay, that is better. We will not analyse any data near to the artifact produced by downsampling.\n","\n","Now we have some sensible data to analyse so let's get right on with doing the analysis!"]},{"cell_type":"markdown","metadata":{"id":"UeYZNhjjCRP7"},"source":["## Low dimensional analysis\n","\n","In general a compact binary coalescence signal is described by 15 parameters describing the masses, spins, orientation, and position of the two compact objects along with a time at which the signal merges. The goal of parameter estimation is to figure out what the data (and any cogent prior information) can tell us about the likely values of these parameters. This is called the *posterior distribution* of the parameters.\n","\n","To start with, we analyse the data fixing all but a few of the parameters to known values (in other words the priors of these parameters are delta functions), this will enable us to run things in a few minutes rather than the many hours needed to do full parameter estimation.\n","\n","We start by thinking about the mass of the system. We call the heavier black hole the primary and label its mass $m_1$ and that of the secondary (lighter) black hole $m_2$. In this way, we always define $m_1 \\ge m_2$. It turns out that inferences about $m_1$ and $m_2$ are highly correlated, we will see exactly what this means later on.\n","\n","Bayesian inference methods are powerful at figuring out highly correlated posteriors. But, we can ease the process by sampling in parameters which are not highly correlated. In particular, we define two new mass parameter to be the [chirp mass](https://en.wikipedia.org/wiki/Chirp_mass)\n","\n","$$ \\mathcal{M} = \\frac{(m_1 m_2)^{3/5}}{(m_1 + m_2)^{1/5}} $$\n","\n","and the mass ratio\n","\n","$$ q = \\frac{m_{2}}{m_1}\\,. $$\n","\n","If we sample (make inferences about) $\\mathcal{M}$ and $q$, our code is much faster than if we use $m_1$ and $m_2$ directly! Note that so long as the equivalent prior is given, one can also sample in the component masses themselves and you will get the same answer, it is just much slower!\n","\n","Once we have inferred $\\mathcal{M}$ and $q$, we can then derive $m_1$ and $m_2$ from the resulting samples, as we will see below.\n","\n","Let's run a short (~1min on a single 2.8GHz core), low-dimensional parameter estimation analysis. This is done by defining a prior dictionary where all parameters are fixed, except those that we want to vary."]},{"cell_type":"markdown","metadata":{"id":"alMBEjPdCRP8"},"source":["### Create a prior\n","\n","Here, we create a prior fixing everything except the chirp mass, mass ratio, phase and geocent_time parameters to fixed values. The first two were described above. The second two give the phase of the system and the time at which it merges."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jk9nFtasCRP9"},"outputs":[],"source":["prior = bilby.core.prior.PriorDict()\n","prior['chirp_mass'] = Uniform(name='chirp_mass', minimum=30.0,maximum=32.5)\n","prior['mass_ratio'] = Uniform(name='mass_ratio', minimum=0.5, maximum=1)\n","prior['phase'] = Uniform(name=\"phase\", minimum=0, maximum=2*np.pi)\n","prior['geocent_time'] = Uniform(name=\"geocent_time\", minimum=time_of_event-0.1, maximum=time_of_event+0.1)\n","prior['a_1'] =  0.0\n","prior['a_2'] =  0.0\n","prior['tilt_1'] =  0.0\n","prior['tilt_2'] =  0.0\n","prior['phi_12'] =  0.0\n","prior['phi_jl'] =  0.0\n","prior['dec'] =  -1.2232\n","prior['ra'] =  2.19432\n","prior['theta_jn'] =  1.89694\n","prior['psi'] =  0.532268\n","prior['luminosity_distance'] = 412.066"]},{"cell_type":"markdown","metadata":{"id":"paWKNNyFCRP_"},"source":["## Create a likelihood\n","\n","For Bayesian inference, we need to evaluate the likelihood. In Bilby, we create a **likelihood object**. This is the communication interface between the sampling part of Bilby and the data. Explicitly, when Bilby is sampling it only uses the `parameters` and `log_likelihood()` of the likelihood object. This means the likelihood can be arbitrarily complicated and the sampling part of Bilby will not mind a bit!\n","\n","Let's create a `GravitationalWaveTransient`, a special inbuilt method carefully designed to wrap up evaluating the likelihood of a waveform model in some data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQELYEpMCRQB"},"outputs":[],"source":["# First, put our \"data\" created above into a list of intererometers\n","# (the order is arbitrary)\n","interferometers = [H1, L1]\n","\n","# Next create a dictionary of arguments to pass to LALSimulation (the part of\n","# LALSuite that calculates waveforms) via the Bilby waveform generator interface.\n","# This is where we specify the waveform approximant.\n","waveform_arguments = dict(\n","    waveform_approximant='IMRPhenomPv2', reference_frequency=100., catch_waveform_errors=True)\n","\n","# Next, create a waveform_generator object. This wraps up some of the jobs of\n","# converting between parameters, etc.\n","waveform_generator = bilby.gw.WaveformGenerator(\n","    frequency_domain_source_model=bilby.gw.source.lal_binary_black_hole,\n","    waveform_arguments=waveform_arguments,\n","    parameter_conversion=convert_to_lal_binary_black_hole_parameters)\n","\n","# Finally, create the likelihood, passing in what is needed to get going\n","likelihood = bilby.gw.likelihood.GravitationalWaveTransient(\n","    interferometers,\n","    waveform_generator,\n","    priors=prior,\n","    time_marginalization=True,\n","    phase_marginalization=True,\n","    distance_marginalization=False)"]},{"cell_type":"markdown","metadata":{"id":"dAxKafx-CRQC"},"source":["Note that we also specify `time_marginalization=True` and `phase_marginalization=True`. This is a trick often used in Bayesian inference. We analytically marginalize (integrate) over the time/phase of the system while sampling, effectively reducing the parameter space and making it easier to sample. Bilby will then figure out (after the sampling) posteriors for these marginalized parameters. For an introduction to this topic, see [Thrane & Talbot (2019)](https://arxiv.org/abs/1809.02293)."]},{"cell_type":"markdown","metadata":{"id":"3xtoR-9RCRQD"},"source":["### Run the analysis"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"LCfygeVyygvM"},"source":["Now that the prior is set-up and the likelihood is set-up (with the data and the signal mode), we can run the sampler to get the posterior result. This function takes the likelihood and prior along with some options for how to do the sampling and how to save the data."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"HHS9JSX3ygvN"},"outputs":[],"source":["result_short = bilby.run_sampler(\n","    likelihood, prior, sampler='dynesty', outdir='short', label=\"GW150914\",\n","    conversion_function=bilby.gw.conversion.generate_all_bbh_parameters,\n","    sample=\"unif\", nlive=500, dlogz=3  # <- Arguments are used to make things fast - not recommended for general use\n",")"]},{"cell_type":"markdown","metadata":{"id":"ziijqUI4CRQD"},"source":["### Looking at the outputs"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"wKR045TIygvT"},"source":["The `run_sampler` returned `result_short`, a Bilby result object. The posterior samples are stored in a [pandas data frame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) (think of this like a spreadsheet); let's take a look at it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K93RXtiTCRQE"},"outputs":[],"source":["result_short.posterior"]},{"cell_type":"markdown","metadata":{"id":"RrOwxDxFCRQE"},"source":["We can pull out specific parameters that we are interested in"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrnqDXKMCRQF"},"outputs":[],"source":["result_short.posterior[\"chirp_mass\"]"]},{"cell_type":"markdown","metadata":{"id":"ETVN6xEnCRQF"},"source":["This returned another `pandas` object. If you just want to get the numbers as a numpy array run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WmScbpXtCRQF"},"outputs":[],"source":["Mc = result_short.posterior[\"chirp_mass\"].values"]},{"cell_type":"markdown","metadata":{"id":"hhO7xoBwCRQF"},"source":["We can then get some useful quantities such as the 90\\% credible interval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26jXrUT0CRQF"},"outputs":[],"source":["lower_bound = np.quantile(Mc, 0.05)\n","upper_bound = np.quantile(Mc, 0.95)\n","median = np.quantile(Mc, 0.5)\n","print(\"Mc = {:.1f} with a 90% C.I = {:.1f} -> {:.1f}\".format(median, lower_bound, upper_bound))"]},{"cell_type":"markdown","metadata":{"id":"0C7BedDzCRQF"},"source":["We can then plot the chirp mass in a histogram adding a region to indicate the 90\\% C.I."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ysIXFZdCRQF"},"outputs":[],"source":["fig, ax = plt.subplots()\n","ax.hist(result_short.posterior[\"chirp_mass\"], bins=20)\n","ax.axvspan(lower_bound, upper_bound, color='C1', alpha=0.4)\n","ax.axvline(median, color='C1')\n","ax.set_xlabel(\"chirp mass\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NasEkBhoCRQF"},"source":["The result object also has in-built methods to make nice plots such as corner plots. You can add the priors if you are only plotting parameters which you sampled in, e.g."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPgOJmFZCRQG"},"outputs":[],"source":["result_short.plot_corner(parameters=[\"chirp_mass\", \"mass_ratio\", \"geocent_time\", \"phase\"], prior=True)"]},{"cell_type":"markdown","metadata":{"id":"DQfwkWDBCRQG"},"source":["You can also plot lines indicating specific points. Here, we add the values recorded on [GWOSC](https://www.gw-openscience.org/events/GW150914/). Notably, these fall outside the bulk of the posterior uncertainty here. This is because we limited our prior. If instead we were to run the full analysis, these would agree nicely."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"SB4AqmTaygvU"},"outputs":[],"source":["parameters = dict(mass_1=36.2, mass_2=29.1)\n","result_short.plot_corner(parameters)"]},{"cell_type":"markdown","metadata":{"id":"HwzwmpjbCRQG"},"source":["In this plot we start to see the correlation between $m_1$ and $m_2$ that we disucssed earlier."]},{"cell_type":"markdown","metadata":{"id":"X-haU8sdCRQG"},"source":["### Meta data\n","The result object also stores meta data, such as the priors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VOR_wSpYCRQG"},"outputs":[],"source":["result_short.priors"]},{"cell_type":"markdown","metadata":{"id":"MKeHN8SXCRQG"},"source":["and details of the analysis itself:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEyszqpfCRQG"},"outputs":[],"source":["result_short.sampler_kwargs[\"nlive\"]"]},{"cell_type":"markdown","metadata":{"id":"6kWNnYZTCRQG"},"source":["Finally, we can also extract the **Bayes factor** for the signal vs. Gaussian noise. This quantifies the probability that the analyzed segment constains a binary black hole signal compared to just containing noise."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTIR8tg2CRQG"},"outputs":[],"source":["print(\"ln Bayes factor = {:.2f} +/- {:.2f}\".format(\n","    result_short.log_bayes_factor, result_short.log_evidence_err))"]},{"cell_type":"markdown","source":["### Other attributes of the result"],"metadata":{"id":"4AQjjd0x7led"}},{"cell_type":"code","source":["print(result_short.log_evidence)\n","print(result_short.log_evidence_err)\n","print(result_short.log_noise_evidence)\n","print(result_short.log_bayes_factor)\n","print(result_short.log_evidence - result_short.log_noise_evidence)"],"metadata":{"id":"1iytHdHq7u_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(result_short.log_likelihood_evaluations)"],"metadata":{"id":"2LyY987S71_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, _, histo = plt.hist(result_short.log_likelihood_evaluations, bins=100)"],"metadata":{"id":"ZIJx4HMh75GL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(result_short.covariance_matrix)\n","\n","#print(result_short.nested_samples)\n","#print(result_short.samples)"],"metadata":{"id":"oOkQ0KaeAqok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["qty = 'chirp_mass'\n","#quantiles = (0.16, 0.84)\n","quantiles = (0.05, 0.95)\n","result_short.plot_single_density(qty, save=False, prior=result_short.priors[qty], quantiles=quantiles)\n","plt.yscale('log')\n","result_short.plot_single_density(qty, save=False, quantiles=quantiles, cumulative=True)\n","plt.yscale('linear')"],"metadata":{"id":"4OQ6_mHR8AwG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Markdown as md\n","md(result_short.get_one_dimensional_median_and_error_bar(qty, quantiles=quantiles).string)"],"metadata":{"id":"XkMSbAcK7_JA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_short.posterior_volume"],"metadata":{"id":"OS97CbhL79Nz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"AgbBCReY7LGQ"},"execution_count":null,"outputs":[]}]}