{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyCBC.ipynb","provenance":[],"collapsed_sections":["_aI2xJ4qzarB","vjYaibyMEjT8","d9O1zsAO1zul","omeA9c5XCLJJ"],"authorship_tag":"ABX9TyMEXVJfIYhO5SWHDF2a06su"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<img style=\"float: left;padding: 1.3em\" src=\"https://indico.in2p3.fr/event/18313/logo-786578160.png\">  \n","\n","This notebook puts together tutorials available at the [Gravitational-Wave Open Science Center (GWOSC) website](https://www.gw-openscience.org)\n","\n","Topics:\n","* Generating (and plotting) gravitational waveforms emitted by compact binary coalescences (CBCs)\n","* Matched filtering to identify compact object mergers\n","* Working with compact object merger parameters and waveforms"],"metadata":{"id":"P8voDYqCETCg"}},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"_aI2xJ4qzarB"},"source":["# Part 2.1: Generating Waveforms\n","\n","Let's see how to numerically obtain the gravitational waveform radiated during a compact binary coalescence, assuming the basic parameters of the binary are known.\n","\n","We will be using the [PyCBC](http://github.com/ligo-cbc/pycbc) library, which provides an easy-to-use **Python** interface to obtain such waveforms. PyCBC can more generally be used to analyze or simulate gravitational-wave data, find or simulate astrophysical signals from compact binary mergers, and study their parameters. It is one of the tools routinely used by groups within and outside of the LIGO and Virgo collaborations. Additional [examples](http://pycbc.org/pycbc/latest/html/#library-examples-and-interactive-tutorials) and module level documentation are [here](http://pycbc.org/pycbc/latest/html/py-modindex.html)."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"1wcq0imbzarE"},"source":["##  Software installation\n","\n","PyCBC is installable through pip. It relies on portions of the [LALSuite](https://lscsoft.docs.ligo.org/lalsuite/)(LIGO Algorithm Library) **C-library**. A bundled version of this suitable for use with PyCBC is also available on Mac / Linux through pip. **It is recommended** to use [conda](https://docs.ligo.org/lscsoft/conda/) on your own machine, as explained in the [installation instructions](https://github.com/gw-odw/odw-2019/blob/master/setup.md). This usage might look a little different than normal, simply because we want to do this directly from the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"UfyiwG64zarG"},"outputs":[],"source":["# -- Uncomment following line if running in Google Colab\n","! pip install -q 'PyCBC==1.18.0' 'lalsuite==6.82'"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"wHDzqNw3zarK"},"source":["**Important:** With Google Colab, you may need to restart the runtime after running the cell above."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"HwA6949mzarL"},"source":["## Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"0brf1i5ZzarM"},"outputs":[],"source":["%matplotlib inline\n","\n","from pycbc.waveform import get_td_waveform\n","import pylab"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"u9i3C2HAzarT"},"source":["## Generate your first waveform!\n","\n","We will generate the gravitational waveform using one of the available _waveform approximants_, that is, \"recipes\" that take the binary masses, spins, etc. as in put and yield the corresponding gravitational waveform.\n","\n","The waveform can be generated as a time series using [`get_td_waveform()`](http://pycbc.org/pycbc/latest/html/pycbc.waveform.html#pycbc.waveform.waveform.get_td_waveform). There are some additional examples using this interface [here](http://pycbc.org/pycbc/latest/html/waveform.html). The key parameters are the masses of the binary (given in solar masses), the time between samples (in seconds), the starting gravitational-wave frequency (Hz) and the name of the approximant to use. A variety of approximants are available: these include different physical effects, rely on different approximations, etc.\n","\n","In this example, we use the 'IMRPhenomNSBH' approximant. This is an implementation of the model introduced [in this paper](https://arxiv.org/pdf/2002.08383.pdf). It models the gravitational waveform of the inspiral and merger of a neutron star and a black holes; it includes the ability for the black hole to spin in the same direction as the orbit (aligned spin) and it includes tidal effects for the neutron star.\n","\n","[ _There are many other models available, with different methodologies employed and physical effects modelled._ ]"]},{"cell_type":"code","source":["# The outputs of this function are the \"plus\" and \"cross\" polarizations \n","# of the gravitational-wave signal as viewed from the line of sight at \n","# a given source inclination (assumed face-on, i.e. zero inclination\n","# if not provided)\n","hp, hc = get_td_waveform(approximant=\"IMRPhenomNSBH\",\n","                         mass1=4,\n","                         mass2=1.4,\n","                         delta_t=1.0/16384,\n","                         f_lower=30,\n","                         lambda2=1000)\n","\n","pylab.figure(figsize=pylab.figaspect(0.4))\n","pylab.plot(hp.sample_times, hp, label='Plus Polarization')\n","pylab.plot(hp.sample_times, hc, label='Cross Polarization')\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Strain')\n","pylab.legend()\n","pylab.grid()\n","pylab.show()\n","\n","# Zoom in near the merger time\n","pylab.figure(figsize=pylab.figaspect(0.4))\n","pylab.plot(hp.sample_times, hp, label='Plus Polarization')\n","pylab.plot(hp.sample_times, hc, label='Cross Polarization')\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Strain')\n","pylab.xlim(-.01, .01)\n","pylab.legend()\n","pylab.grid()\n","pylab.show()"],"metadata":{"id":"d8RVbozPBlPY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"aiujHxRnzarW"},"source":["We can see that in the this case, the two polarizations differ only by the phase of the signal. This is a known property of the signal, when the orbital plane of the binary does not precess (i.e. the individual black holes spins are aligned with the orbital angular momentum). In the zoom-in plot, we can see the merger itself and the ringdown that follows."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"mq4ZxvT1zarX"},"source":["## How does the waveform change with the deformation parameter of the neutron star?\n","\n","Below you can see how the phase and amplityde of the waveform vary with the deformability parameter ($\\Lambda_2$) of the lower-mass object, i.e., the neutron star."]},{"cell_type":"code","source":["# Component mass of each binary component. We'll simplify here and assume that each \n","# component of the binary has the same mass. Again, units are in solar masses.\n","pylab.figure(figsize=pylab.figaspect(0.4))\n","for l in [0, 500, 1000, 2000]:\n","    hp, hc = get_td_waveform(approximant=\"IMRPhenomNSBH\",\n","                         mass1=4,\n","                         mass2=1.4,\n","                         delta_t=1.0/32768,\n","                         f_lower=30,\n","                         lambda2=l)\n","    \n","    pylab.plot(hp.sample_times, hp, label='$\\Lambda_2=%s$' % l)\n","pylab.legend()\n","pylab.grid()\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Strain')\n","# Zoom in near the merger time\n","pylab.xlim(-.01, .005)\n","pylab.show()"],"metadata":{"id":"wTP3EPr4CmJB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"G8WZHYc7zara"},"source":["## Changing the distance of the waveform\n","\n","The luminosity distance of the source is also a parameter when you generate a waveform. The units used are Megaparsecs (i.e. $10^6\\mathrm{\\,pc}$). Keep in mind that no redshift effects are taken into account here, so there is a simple linear relationship between distance and amplitude."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"8CjpDbxtzarb"},"outputs":[],"source":["pylab.figure(figsize=pylab.figaspect(0.4))\n","for d in [100, 500, 1000]:\n","    hp, hc = get_td_waveform(approximant=\"IMRPhenomNSBH\",\n","                         mass1=4,\n","                         mass2=1.4,\n","                         delta_t=1.0/32768,\n","                         f_lower=30,\n","                         lambda2=500,\n","                         distance=d)\n","    \n","    pylab.plot(hp.sample_times, hp, label='Distance$=%s$ Mpc' % d)\n","pylab.legend()\n","pylab.grid()\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Strain')\n","pylab.show()"]},{"cell_type":"markdown","source":["# Part 2.2: PyCBC â€“ An Introduction to Matched-Filtering\n","\n","We will be using the [PyCBC](http://github.com/ligo-cbc/pycbc) library, which is used to study gravitational-wave data, find astrophysical sources due to compact binary mergers, and study their parameters. These are some of the same tools that the LIGO and Virgo collaborations use to find gravitational waves in LIGO/Virgo data \n","\n","We will walk through how to find a specific signal in LIGO data. We present matched filtering as a cross-correlation, in both the time domain and the frequency domain. In Part 2.3, we use the method as encoded in PyCBC, which is optimal in the case of Gaussian noise and a known signal model. In reality our noise is not entirely Gaussian, and in practice we use a variety of techniques to separate signals from noise in addition to the use of the matched filter. \n","\n","Additional [examples](http://pycbc.org/pycbc/latest/html/#library-examples-and-interactive-tutorials) and module level documentation are [here](http://pycbc.org/pycbc/latest/html/py-modindex.html)."],"metadata":{"id":"vjYaibyMEjT8"}},{"cell_type":"markdown","source":["## Initialization"],"metadata":{"id":"H6Lc55GpFPAI"}},{"cell_type":"code","source":["import numpy"],"metadata":{"id":"j7WnkaSSFRbL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"n1n6Ut_v0psP"},"source":["## Matched-filtering: Finding well modelled signals in Gaussian noise\n","\n","Matched filtering can be shown to be the optimal method for \"detecting\" _known_ signals in _Gaussian_ noise. We explore these two assumptions a little later, but for now let's demonstrate how this works.\n","\n","Let's assume you have a stretch of noise, white noise to start:"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"YUS9v3eM0psQ"},"outputs":[],"source":["# Specify the sampling rate.\n","# LIGO raw data is sampled at 16384 Hz (=2^14 samples/second).\n","# It captures signal frequency content up to f_Nyquist = 8192 Hz.\n","# Here, we will make the computation faster by sampling at a lower rate.\n","sample_rate = 1024 # samples per second\n","data_length = 1024 # seconds\n","\n","# Generate a long stretch of white noise: the data series and the time series.\n","data = numpy.random.normal(size=[sample_rate * data_length])\n","times = numpy.arange(len(data)) / float(sample_rate)"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"Lb1qfZuj0psU"},"source":["Let's add a gravitational wave signal to some random part of this data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lfoqA13HEe2V"},"outputs":[],"source":["# The \"approximant\" (jargon for parameterized waveform family).\n","# IMRPhenomD (a phenomenological Inspiralâ€“Mergerâ€“Ringdown wafeform model) is defined in the frequency domain, but we will get it in the time domain (td).\n","# It is quick to run, but it does not include effects such as non-aligned component spin, or higher order modes.\n","apx = 'IMRPhenomD'"]},{"cell_type":"markdown","metadata":{"id":"QqSW07zoEe2V"},"source":["You can specify [many parameters](https://pycbc.org/pycbc/latest/html/pycbc.waveform.html?highlight=get_td_waveform#pycbc.waveform.waveform.get_td_waveform), but here we use defaults for everything except the masses.\n","\n","`get_td_waveform` returns both $h_+$ and $h_{\\times}$, but we only use $h_+$ for now."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKYRXAMsEe2W"},"outputs":[],"source":["hp1, _ = get_td_waveform(approximant=apx,\n","                         mass1=10,\n","                         mass2=10,\n","                         delta_t=1.0/sample_rate,\n","                         f_lower=25)"]},{"cell_type":"markdown","metadata":{"id":"omiTkH2WEe2W"},"source":["The amplitude of gravitational-wave signals is normally of order $10^{-20}$. To demonstrate our method on white noise with amplitude $O(1)$ we normalize our signal so the cross-correlation of the signal with itself will give a value of 1.\n","\n","In this case we can interpret the cross-correlation of the signal with white noise as a signal-to-noise ratio."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZoo3wl6Ee2W"},"outputs":[],"source":["hp1 = hp1 / max(numpy.correlate(hp1, hp1, mode='full'))**0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"8iMaIs1d0psW"},"outputs":[],"source":["# Note that in this figure, per the construction above, the waveform amplitude is of order 1.\n","# The duration (for frequency above f_lower=25 Hz) is only 3 or 4 seconds long.\n","# The waveform is \"tapered\": slowly ramped up from zero to full strength, over the first second or so.\n","# It is zero-padded at earlier times.\n","pylab.figure()\n","pylab.title(\"The waveform hp1\")\n","pylab.plot(hp1.sample_times, hp1)\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Normalized amplitude')\n","\n","# Shift the waveform to start at a random time in the Gaussian noise data.\n","waveform_start = numpy.random.randint(0, len(data) - len(hp1))\n","data[waveform_start:waveform_start+len(hp1)] += 10 * hp1.numpy()\n","\n","pylab.figure()\n","pylab.title(\"Looks like random noise, right?\")\n","pylab.plot(hp1.sample_times, data[waveform_start:waveform_start+len(hp1)])\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Normalized amplitude')\n","\n","pylab.figure()\n","pylab.title(\"Signal in the data\")\n","pylab.plot(hp1.sample_times, data[waveform_start:waveform_start+len(hp1)])\n","pylab.plot(hp1.sample_times, 10 * hp1)\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Normalized amplitude')"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"mmAJqGqH0psZ"},"source":["To search for this signal we can cross-correlate the signal with the entire dataset.  This is not optimized in any way at this point, it is just to show the method.\n","\n","We will do the cross-correlation in the time domain, once for each time step. It runs slowly..."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"fi3D6sW70psa"},"outputs":[],"source":["cross_correlation = numpy.zeros([len(data)-len(hp1)])\n","hp1_numpy = hp1.numpy()\n","for i in range(len(data) - len(hp1_numpy)):\n","    cross_correlation[i] = (hp1_numpy * data[i:i+len(hp1_numpy)]).sum()\n","\n","# Plot the cross-correlated data vs time. Superimpose the location of the end of the signal;\n","# this is where we should find a peak in the cross-correlation.\n","pylab.figure()\n","times = numpy.arange(len(data) - len(hp1_numpy)) / float(sample_rate)\n","pylab.plot(times, cross_correlation)\n","pylab.plot([waveform_start/float(sample_rate), waveform_start/float(sample_rate)], [-10,10],'r:')\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Cross-correlation')"]},{"cell_type":"markdown","metadata":{"id":"Vh6Tmsa5Ee2X"},"source":["## Detection in Colored Noise"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"cafBZG960psc"},"source":["Here you can see that the largest spike from the cross-correlation comes at the time of the signal. We only really need one more ingredient to describe matched-filtering: **\"colored\" noise** (Gaussian noise but with a frequency-dependent variance; white noise has frequency-independent variance). \n","\n","Let's repeat the process, but generate a stretch of data colored with LIGO's _zero-detuned--high-power_ noise curve. We'll use a PyCBC library to do this."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"9Jcw-P_v0psd"},"outputs":[],"source":["# http://pycbc.org/pycbc/latest/html/noise.html\n","import pycbc.noise\n","import pycbc.psd\n","\n","# The color of the noise matches a PSD which you provide:\n","# Generate a PSD matching Advanced LIGO's zero-detuned--high-power noise curve \n","flow = 10.0\n","delta_f = 1.0 / 128\n","flen = int(sample_rate / (2 * delta_f)) + 1\n","psd = pycbc.psd.aLIGOZeroDetHighPower(flen, delta_f, flow)\n","\n","# Generate colored noise\n","delta_t = 1.0 / sample_rate\n","ts = pycbc.noise.noise_from_psd(data_length*sample_rate, delta_t, psd, seed=127)\n","\n","# Estimate the amplitude spectral density (ASD = sqrt(PSD)) for the noisy data \n","# using the \"Welch\" method. We choose 4 seconds PSD samples that are overlapped 50%\n","seg_len = int(4 / delta_t)\n","seg_stride = int(seg_len / 2)\n","estimated_psd = pycbc.psd.welch(ts,seg_len=seg_len,seg_stride=seg_stride)\n","\n","# Plot it\n","pylab.loglog(estimated_psd.sample_frequencies, estimated_psd, label='estimate')\n","pylab.loglog(psd.sample_frequencies, psd, linewidth=3, label='known psd')\n","pylab.xlim(xmin=flow, xmax=512)\n","pylab.ylim(1e-47, 1e-45)\n","pylab.xlabel('Frequency [Hz]')\n","pylab.ylabel('Power spectral density')\n","pylab.legend()\n","pylab.grid()\n","pylab.show()\n","\n","# Add the signal, this time, with a \"typical\" amplitude.\n","ts[waveform_start:waveform_start+len(hp1)] += hp1.numpy() * 1E-20"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"rSgmyob_0psf"},"source":["Then all we need to do is to \"**whiten**\" both the data, and the template waveform. In the frequency domain, this can be done by dividing by the PSD value bin-by-bin. This *can* also be done in the time domain, but it is more intuitive in the frequency domain"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"w_v6gzUh0psg"},"outputs":[],"source":["# Generate a PSD for whitening the data\n","from pycbc.types import TimeSeries\n","\n","# The PSD, sampled properly for the noisy data\n","flow = 10.0\n","delta_f = 1.0 / data_length\n","flen = int(sample_rate / (2 * delta_f)) + 1\n","psd_td = pycbc.psd.aLIGOZeroDetHighPower(flen, delta_f, 0)\n","\n","# The PSD, sampled properly for the signal\n","delta_f = sample_rate / float(len(hp1))\n","flen = int(sample_rate / (2 * delta_f)) + 1\n","psd_hp1 = pycbc.psd.aLIGOZeroDetHighPower(flen, delta_f, 0)\n","\n","# The 0th and Nth values of the PSD and the signal are zero.\n","# Set them to a nearby value to avoid dividing by zero.\n","psd_td[0] = psd_td[1]\n","psd_td[len(psd_td) - 1] = psd_td[len(psd_td) - 2]\n","psd_hp1[0] = psd_hp1[1]\n","psd_hp1[len(psd_hp1) - 1] = psd_hp1[len(psd_hp1) - 2]\n","\n","# Convert both noisy data and the signal to frequency domain,\n","# and divide each by ASD = PSD**0.5, then convert back to time domain.\n","# This \"whitens\" the data and the signal template. \n","# Multiplying the signal template by 1E-21 puts it into realistic units of strain.\n","data_whitened = (ts.to_frequencyseries() / psd_td**0.5).to_timeseries()\n","hp1_whitened = (hp1.to_frequencyseries() / psd_hp1**0.5).to_timeseries() * 1E-21"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"d_HLbCCN0psi"},"outputs":[],"source":["# Now we repeat the correlation calculation, in the time domain, but with whitened data and template.\n","cross_correlation = numpy.zeros([len(data)-len(hp1)])\n","hp1n = hp1_whitened.numpy()\n","datan = data_whitened.numpy()\n","for i in range(len(datan) - len(hp1n)):\n","    cross_correlation[i] = (hp1n * datan[i:i+len(hp1n)]).sum()\n","\n","# Plot the cross-correlation in the time domain. Superimpose the location of the end of the signal.\n","# Note how much bigger the cross-correlation peak is, relative to the noise level,\n","# compared with the unwhitened version of the same quantity. SNR is much higher!\n","pylab.figure()\n","times = numpy.arange(len(datan) - len(hp1n)) / float(sample_rate)\n","pylab.plot(times, cross_correlation)\n","pylab.plot([waveform_start/float(sample_rate), waveform_start/float(sample_rate)],\n","           [(min(cross_correlation))*1.1,(max(cross_correlation))*1.1],'r:')\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Cross-correlation')"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"DuO18i780pso"},"source":["### Optimizing a matched-filter\n","\n","That is all that a matched-filter is: a cross-correlation of the data with a template waveform performed as a function of time. This cross-correlation walking through the data is a convolution operation. Convolution operations are more optimally performed in the frequency domain, which becomes a $O(N \\ln N)$ operation, as opposed to the $O(N^2)$ operation shown here. You can also conveniently vary the phase of the signal in the frequency domain, as the next part illusyrates. PyCBC implements a frequency-domain matched-filtering engine, which is much faster than the code we saw here."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"d9O1zsAO1zul"},"source":["# Part 2.3: PyCBC â€“ Matched Filtering in Action"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"cJ9NEh-q1zuw"},"source":["## Looking for a specific signal in the data\n","\n","If you know what signal you are looking for in the data, then matched filtering is known to be the optimal method in Gaussian noise to extract the signal.  Even when the parameters of the signal are unkown, one can test for any set of parameters one is interested in finding."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"4rvbMKWW1zux"},"source":["### Preconditioning the Data \n"," \n","The purposes of preconditioning data are:\n","1. to reduce the dynamic range of the data and\n","1. to supress low-frequency behavior that can introduce numerical artefacts.\n","\n","We may also wish to reduce the sample rate of the data if high frequency content is not important.\n","\n","PyCBC contains `pycbc.catalog`, an interface to the [GWOSC catalog](https://www.gw-openscience.org/eventapi/) to easily access the data and parameters of the published gravitational-wave signals."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"8VSW4sca1zuy"},"outputs":[],"source":["from pycbc.catalog import Merger\n","from pycbc.filter import resample_to_delta_t, highpass\n","\n","# As an example we use the GW150914 data\n","merger = Merger(\"GW150914\")\n","\n","# Get the data from the Hanford detector\n","strain = merger.strain('H1')\n","\n","# Remove the low frequency content and downsample the data to 2048Hz\n","strain = highpass(strain, 15.0)\n","strain = resample_to_delta_t(strain, 1.0/2048)\n","\n","pylab.plot(strain.sample_times, strain)\n","pylab.xlabel('Time (s)')\n","pylab.show()"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"sHJcsujM1zu1"},"source":["[ _To read data from a local file instead of from the GWOSC server, we can use the [`pycbc.frame.read_frame(file, channel_name)`](https://github.com/gwastro/pycbc/blob/master/docs/frame.rst) method._ ]"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"qqXqAIMx1zu2"},"source":["### Filter Wraparound \n","\n","Note the spike in the data at the boundaries. This is caused by the highpass and resampling stages filtering the data. When the filter is applied to the boundaries, it wraps around to the beginning of the data. Since the data itself has a discontinuity (i.e., it is not cyclic) the filter itself will ring off for a time up to the length of the filter. \n","\n","Even if a visible transient is not seen, we want to avoid filters that act on times which are not causally connected. To avoid this, we trim the ends of the data sufficiently to ensure that they do not wrap around the input. We will enforce this requirement in all steps of our filtering."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"nthrNAfz1zu2"},"outputs":[],"source":["# Remove 2 seconds of data from both the beginning and end\n","conditioned = strain.crop(2, 2)\n","\n","pylab.plot(conditioned.sample_times, conditioned)\n","pylab.xlabel('Time (s)')\n","pylab.show()"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"1oVFXwS-1zu5"},"source":["### Calculate the Power Spectral Density\n","\n","Optimal matched filtering requires weighting the frequency components of the potential signal and data by the noise amplitude. We can view this as filtering the data with the time series equivalent of 1 / PSD. To ensure that we can control the effective length of the filter, we window the time domain equivalent of the PSD to a specific length. This has the effect of losing some information about line behavior in the detector. However, since our signals span a large frequency range, and lines are narrow, this is a negligible effect.\n","\n","**Important note:** _computing a PSD from data that might contain signals, non-Gaussianities and non-stationarities is not trivial. In this example we use Welch's method to obtain a PSD estimate. PyCBC's PSD module contains tools for measuring PSDs, or directly using pre-generated PSDs._"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"lBNv_Jod1zu5"},"outputs":[],"source":["from pycbc.psd import interpolate, inverse_spectrum_truncation\n","\n","# Estimate the power spectral density\n","\n","# We use 4 second samples of our time series in Welch method.\n","psd = conditioned.psd(4)\n","\n","# Now that we have the psd we need to interpolate it to match our data\n","# and then limit the filter length of 1 / PSD. After this, we can\n","# directly use this PSD to filter the data in a controlled manner\n","psd = interpolate(psd, conditioned.delta_f)\n","\n","# 1/PSD will now act as a filter with an effective length of 4 seconds\n","# Since the data has been highpassed above 15 Hz, and will have low values\n","# below this we need to inform the function to not include frequencies\n","# below this frequency. \n","psd = inverse_spectrum_truncation(psd, int(4 * conditioned.sample_rate),\n","                                  low_frequency_cutoff=15)"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"ScKiCSQv1zu7"},"source":["### Make your Signal Model\n","\n","Conceptually, matched filtering involves laying the potential signal over your data and integrating (after weighting frequencies correctly).\n","If there is a signal in the data that aligns with your \"template\", you will get a large value when integrated over.\n","\n","In this case we \"know\" what the signal parameters are. In a real search we would grid over the parameters and calculate the SNR time series for each grid point.\n","\n","We will assume equal masses, and non-rotating black holes which is within the posterior probability of GW150914."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oz9VgH-2Jcn0"},"outputs":[],"source":["m = 36 # Solar masses\n","hp, hc = get_td_waveform(approximant=\"SEOBNRv4_opt\",\n","                     mass1=m,\n","                     mass2=m,\n","                     delta_t=conditioned.delta_t,\n","                     f_lower=20)\n","\n","# Resize the vector to match our data\n","hp.resize(len(conditioned))"]},{"cell_type":"markdown","metadata":{"id":"pvU8SJmLJcn1"},"source":["The waveform begins at the start of the vector, so if we want the SNR time series to correspond to the approximate merger location, we need to shift the data so that the merger is approximately at the first bin of the data.\n","\n","The `cyclic_time_shift` method shifts the timeseries by a given amount of time.\n","It treats the data as if it were on a ring so points shifted off the end of the series reappear at the start. \n","\n","Note that time stamps are *not* in general affected (as the start time of the full array is shifted), but the index of each point in the vector is.\n","\n","By convention, waveforms returned from `get_td_waveform` have their **merger time stamped with time zero**, so we can use the start time to shift the merger into position."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"iDa3oHLc1zu8"},"outputs":[],"source":["# Let's plot the signal before and after shifting\n","pylab.figure()\n","pylab.title('Before shifting')\n","pylab.plot(hp.sample_times, hp)\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Strain')\n","\n","template = hp.cyclic_time_shift(hp.start_time)\n","\n","pylab.figure()\n","pylab.title('After shifting')\n","pylab.plot(template.sample_times, template)\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Strain')"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"eq3a8qI71zvB"},"source":["### Calculating the Signal-to-Noise Time Series\n","\n","We now calculate the signal-to-noise time series for our template.\n","We take care to handle issues of filter corruption / wraparound by truncating the output time series.\n","We need to account for both the length of the template and 1 / PSD."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"5D-a0-iu1zvC","scrolled":true},"outputs":[],"source":["from pycbc.filter import matched_filter\n","\n","snr = matched_filter(template, conditioned,\n","                     psd=psd, low_frequency_cutoff=20)\n","\n","# Remove time corrupted by the template filter and the psd filter\n","# We remove 4 seonds at the beginning and end for the PSD filtering\n","# And we remove 4 additional seconds at the beginning to account for\n","# the template length (this is somewhat generous for \n","# so short a template). A longer signal such as from a BNS, would \n","# require much more padding at the beginning of the vector.\n","snr = snr.crop(4 + 4, 4)\n","\n","# Why are we taking an abs() on snr here?\n","# The `matched_filter` function actually returns a 'complex' SNR.\n","# What that means is that the real portion correponds to the SNR\n","# associated with directly filtering the template with the data.\n","# The imaginary portion corresponds to filtering with a template that\n","# is 90 degrees out of phase. Since the phase of a signal may be \n","# anything, we choose to maximize over the phase of the signal.\n","pylab.figure(figsize=[10, 4])\n","pylab.plot(snr.sample_times, abs(snr))\n","pylab.ylabel('Signal-to-noise')\n","pylab.xlabel('Time (s)')\n","pylab.show()\n","\n","peak = abs(snr).numpy().argmax()\n","snrp = snr[peak]\n","time = snr.sample_times[peak]\n","\n","print(\"We found a signal at {}s with SNR {}\".format(time, \n","                                                    abs(snrp)))"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"IKZl57RG1zvE"},"source":["## Aligning and Subtracting the Proposed Signal\n","\n","So far, we found a peak in the signal-to-noise for a proposed binary black hole merger.\n","We can use this SNR peak to align our proposal to the data, and to also subtract our proposal from the data."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"iP2NKJ8h1zvF"},"outputs":[],"source":["from pycbc.filter import sigma\n","\n","# The time, amplitude, and phase of the SNR peak tell us how to align\n","# our proposed signal with the data.\n","\n","# Shift the template to the peak time\n","dt = time - conditioned.start_time\n","aligned = template.cyclic_time_shift(dt)\n","\n","# Scale the template so that it would have SNR 1 in this data\n","aligned /= sigma(aligned, psd=psd, low_frequency_cutoff=20.0)\n","\n","# Scale the template amplitude and phase to the peak value\n","aligned = (aligned.to_frequencyseries() * snrp).to_timeseries()\n","aligned.start_time = conditioned.start_time"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"QZ5rXcyC1zvH"},"source":["### Visualize the Overlap Between the Signal and Data\n","\n","To compare the data an signal on equal footing, and to concentrate on the frequency range that is important, we whiten both the template and the data, and then **bandpass** both between 30-300 Hz. In this way, any signal that is in the data is transformed in the same way that the template is."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"BtGiJC651zvI","scrolled":true},"outputs":[],"source":["# We do it this way so that we can whiten both the template and the data\n","white_data = (conditioned.to_frequencyseries() / psd**0.5).to_timeseries()\n","white_template = (aligned.to_frequencyseries() / psd**0.5).to_timeseries()\n","\n","white_data = white_data.highpass_fir(30., 512).lowpass_fir(300, 512)\n","white_template = white_template.highpass_fir(30, 512).lowpass_fir(300, 512)\n","\n","# Select the time around the merger\n","white_data = white_data.time_slice(merger.time-.2, merger.time+.1)\n","white_template = white_template.time_slice(merger.time-.2, merger.time+.1)\n","\n","pylab.figure(figsize=[15, 3])\n","pylab.plot(white_data.sample_times, white_data, label=\"Data\")\n","pylab.plot(white_template.sample_times, white_template, label=\"Template\")\n","pylab.legend()\n","pylab.show()"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"rE7zEPsq1zvK"},"source":["### Subtracting the Signal from the Data\n","\n","Now that we've aligned the template we can simply subtract it. Let's see below how that looks in the time-frequency plots!"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"6IwKtDRz1zvK"},"outputs":[],"source":["subtracted = conditioned - aligned\n","\n","# Plot the original data and the subtracted signal data\n","for data, title in [(conditioned, 'Original H1 Data'),\n","                    (subtracted, 'Signal Subtracted from H1 Data')]:\n","\n","    t, f, p = data.whiten(4, 4).qtransform(.001, logfsteps=100, qrange=(8, 8), frange=(20, 512))\n","    pylab.figure(figsize=[15, 3])\n","    pylab.title(title)\n","    pylab.pcolormesh(t, f, p**0.5, vmin=1, vmax=6)\n","    pylab.yscale('log')\n","    pylab.xlabel('Time (s)')\n","    pylab.ylabel('Frequency (Hz)')\n","    pylab.xlim(merger.time - 2, merger.time + 1)\n","    pylab.show()"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"omeA9c5XCLJJ"},"source":["# Part 2.4: PyCBC â€“ Signal Consistency and Significance\n","\n","We now walk through finding a peak in a noisy signal and estimating its significance given a simplified search. Some assumptions will be noted along the way. We will also make use of one of the standard signal consistency tests to help remove some non-Gaussian transient noise from the background."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"dCZ7ctknCLJT"},"source":["### Significance of Virgo SNR peak of GW170814 ###\n","\n","We will estimate the significance of signal-to-noise peak observed in the Virgo instrument near in time to the large peaks observed in the LIGO-Hanford and LIGO-Livingston observatories.\n","\n","For this purpose we will consider a gravitational-wave signal, the existence of which has been confirmed based on the signals from LIGO detectors alone. This was in fact the case for the matched-filtering based analyses of GW170814, as they did not incorporate any information from the Virgo data.\n","\n","We ask ourselves the following question: *What is the probability that noise can produce a peak as large or larger than the largest peak observed in the Virgo data, and consistent with the lightspeed travel time between all three observatories?*\n","\n","This is a form of null hypothesis testing, where we compute a [$p$-value](https://en.wikipedia.org/wiki/P-value) to answer the question above.\n","\n","<!-- For the purpose of this notebook, we have added a few additional simplifying assumptions, and those will be stated as we go along. -->"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"KTE7hteVCLJU"},"source":["#### Read and Precondition Gravitational Strain Data ####\n","\n","We read in a short segment of data around GW170814, and do some basic preconditioning, as also demonstrated in the previous section. We will also calculate the power spectrum of the data. \n","\n","Notably, here we assume that the power spectrum estimated from the data is constant over the short stretch of time, and is not biased by our choice to center the estimate (very roughly) on the event time. We *do not* assume the data to be stationary, Gaussian, or free from non-astrophysical transient artefacts."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"5z3RSymsCLJV"},"outputs":[],"source":["m = Merger(\"GW170814\")\n","\n","ifos = ['H1', 'L1', 'V1']\n","data = {}\n","psd = {}\n","\n","pylab.figure(figsize=[10, 5])\n","\n","for ifo in ifos:\n","    # Read in and precondition the data\n","    ts = m.strain(ifo).highpass_fir(15, 512)\n","    data[ifo] = resample_to_delta_t(ts, 1.0/2048).crop(2, 2)\n","\n","    # Estimate the power spectral density of the data\n","    # This chooses to use 2s samples in the PSD estimate.\n","    # One should note that the tradeoff in segment length is that\n","    # resolving narrow lines becomes more difficult.\n","    p = data[ifo].psd(2)\n","    p = interpolate(p, data[ifo].delta_f)\n","    p = inverse_spectrum_truncation(p, int(2 * data[ifo].sample_rate), low_frequency_cutoff=15.0)\n","    psd[ifo] = p\n","    \n","    pylab.plot(psd[ifo].sample_frequencies, psd[ifo], label=ifo)\n","\n","pylab.yscale('log')\n","pylab.xscale('log')\n","pylab.ylim(1e-47, 1e-41)\n","pylab.xlim(20, 1024)\n","pylab.ylabel('$Strain^2 / Hz$')\n","pylab.xlabel('Frequency (Hz)')\n","pylab.grid()\n","pylab.legend()\n","pylab.show()"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"n8AQO8sbCLJY"},"source":["#### Generate our template waveform and calculate the Signal-to-noise time series ####\n","\n","To calculate the signal-to-noise time series, we need to generate an estimate of the signal. For this purpose we will assume the source black holes are non spinning, have equal mass, and they agree with the total mass estimate for the system as a whole. [A better method would be to use the maximum likelihood estimate from an analysis of the LIGO data alone, however, what we do here is sufficient for the purposes we have.]"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"SHSxmiY-CLJZ"},"outputs":[],"source":["from pycbc.waveform import get_fd_waveform\n","# Calculate the component mass of each black hole in the detector frame\n","cmass = (m.median1d(\"mass1\")+m.median1d(\"mass2\")) / 2  # This is in the source frame\n","cmass *= (1 + m.median1d(\"redshift\")) # Apply redshift to get to the detector frame\n","\n","# This is a frequency domain waveform generator. It has a very similar syntax to the time domain\n","# waveform function used in prior tutorials. This function returns both a plus and a cross\n","# polarization waveform, but we will just use the plus polarization in building our template\n","# as these are only different by a phase offset in this specific case.\n","hp, _ = get_fd_waveform(approximant=\"IMRPhenomD\",\n","                         mass1=cmass, mass2=cmass,\n","                         f_lower=20.0, delta_f=data[ifo].delta_f)\n","hp.resize(len(psd[ifo]))\n","\n","# For each observatory use this template to calculate the SNR time series\n","snr = {}\n","for ifo in ifos:\n","    snr[ifo] = matched_filter(hp, data[ifo], psd=psd[ifo], low_frequency_cutoff=20)\n","    snr[ifo] = snr[ifo].crop(5, 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"ysNrumWSCLJe"},"outputs":[],"source":["# Show a couple sizes\n","for w, title in [(8, 'Wide View'), (.15, 'Close to GW170814')]:\n","    pylab.figure(figsize=[14, 4])\n","    for ifo in ifos:\n","        pylab.plot(snr[ifo].sample_times, abs(snr[ifo]), label=ifo)\n","\n","    pylab.legend()\n","    pylab.title(title)\n","    pylab.grid()\n","    pylab.xlim(m.time - w, m.time + w)\n","    pylab.ylim(0, 15)\n","    pylab.xlabel('Time (s)')\n","    pylab.ylabel('Signal-to-noise (SNR)')\n","    pylab.show()"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"JDTw54T5CLJh"},"source":["In the SNR time series plots above, we see that while there are nice peaks around GW170814 in each detector, there are also some large peaks at other times. LIGO / Virgo data does contain transient (i.e., limited duration) noise artefacts that an analyses must deal with to search with high sensitivity. One approach for dealing with this is outlined later in this notebook."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"yJ6CIGdACLJi"},"source":["#### How well is the data actually fitting our model? ####\n","\n","One of the ways we can test how well the data actual fits the models to use a $\\chi^2$-based signal consistency test. We employ a version of the test described [in this paper](https://arxiv.org/pdf/gr-qc/0405045.pdf). Schematically, we chop up our template into $p$ number of bins and see how much each contributes to the SNR ($\\rho_i$). We can then calculate our statistic as the difference between the SNR in one bin, and the expected fraction of the total SNR ($\\rho$).\n","\n","$\n","\\chi^2 = \\sum^p_{i=0} (\\rho_i - \\rho / p)^2\n","$\n","\n","This will have $2p-2$ degrees of freedom as each SNR is *complex* representing both possible orthogonal phases the signal could have contributions from. There is also a constraint due to the fact that the sum of the SNRs in all bins must  add up to the total SNR by definition. In this notebook we will normalize this statistic by dividing by the number of degrees of freedom, producing $\\chi^2_r$.\n","\n","We expect that this statistic will be high when the template does not match well with the data, and near unity when the data either is Gaussian noise, or it contains the expected signal in addition to Gaussian noise."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"Zk2SkSZlCLJj"},"outputs":[],"source":["# WARNING!! If you are having problems with this code, replace the import with\n","#from pycbc_chisq import power_chisq\n","from pycbc.vetoes import power_chisq\n","\n","chisq = {}\n","for ifo in ifos:\n","    # The number of bins to use. In principle, this choice is arbitrary. In practice,\n","    # this is empirically tuned.\n","    nbins = 26\n","    chisq[ifo] = power_chisq(hp, data[ifo], nbins, psd[ifo], low_frequency_cutoff=20.0)\n","    chisq[ifo] = chisq[ifo].crop(5, 4)\n","    \n","    dof = nbins * 2 - 2\n","    chisq[ifo] /= dof"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"Pj4cS9DkCLJn","scrolled":true},"outputs":[],"source":["pylab.figure(figsize=[14, 4])\n","\n","for ifo in ifos:\n","    pylab.plot(chisq[ifo].sample_times, chisq[ifo], label=ifo)\n","    \n","pylab.legend()\n","pylab.grid()\n","pylab.xlim(m.time -0.15, m.time + 0.15)\n","pylab.ylim(0, 5)\n","pylab.ylabel('$chi^2_r$')\n","pylab.show()"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"GZhZPz5LCLJq"},"source":["There are some notable features in the $\\chi^2_r$ time series. We see that there is a dip in the value at the time of the peak in the SNR in each observatory. We expect this as the template now aligns with the signal in the data. Also, the values climb just around this minima. This occurs because the template is starting to slide against the true signal in the data but is not perfectly aligned with it."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"mD4UI9ALCLJq"},"source":["#### Re-weighting our SNR to help down-weight times that don't fit our signal ####\n","\n","One approach we can take is to down-weight the times where the data does not appear as either Guassian noise or Gaussian noise + our template. We can do this be combining the SNR time series and our $\\chi^2_r$ time series as follows. This is a method used to re-weight SNR since initial LIGO, and has been employed in the first two Advanced LIGO observing runs. In this tutorial we will choose to rank our events by this statistic. \n","\n","$\\hat{\\rho} = \\frac{\\rho}{ [1 + (\\chi^2_r)^3]^{1/6}}$ where $\\chi^2 > 1$, otherwise $\\rho$\n","\n","For reference on how to rank coincident (i.e., occuring in multiple detector) events in Advanced LIGO, there is a description [here](http://iopscience.iop.org/article/10.3847/1538-4357/aa8f50/pdf)."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"MKJV5tv1CLJs"},"outputs":[],"source":["from pycbc.events.ranking import newsnr\n","\n","# The rho-hat term above is named \"newsnr\" here\n","nsnr = {ifo:newsnr(abs(snr[ifo]), chisq[ifo]) for ifo in ifos}\n","\n","# Show a couple sizes\n","for w, title in [(8, 'Wide View'), (.15, 'Close to GW170814')]:\n","    pylab.figure(figsize=[14, 4])\n","    for ifo in ifos:\n","        pylab.plot(snr[ifo].sample_times, nsnr[ifo], label=ifo)\n","\n","    pylab.legend()\n","    pylab.title(title)\n","    pylab.grid()\n","    pylab.xlim(m.time - w, m.time + w)\n","    pylab.ylim(0, 15)\n","    pylab.xlabel('Time (s)')\n","    pylab.ylabel('Re-weighted Signal-to-noise')\n","    pylab.show()"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"EYp1ZppWCLJw"},"source":["We can see above that there are still peaks around GW170814 in all detectors, at roughly the same signal strength, but that at other times, where that had been peaks in the time series, there are no longer large statistic values."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"6v4oj4T1CLJx"},"source":["#### Calculating the background and significance ####\n","\n","In this section we will determine how significant the peak in the virgo re-weighted SNR time series is. \n","\n","We do this by first determining where one might expect a peak relative to the LIGO observed peaks. This is set by the constraint that an astrophysical source can only cause delays between observatories no larger than the light travel time between them. The [`pycbc.detector.Detector`](http://pycbc.org/pycbc/latest/html/pycbc.html#pycbc.detector.Detector) class provides some convenient methods to ask these sorts of questions.\n","\n","We then calculate the peak in the SNR for this window around the LIGO observed peaks. This is our \"on-source\", or our \"foreground.\"\n","\n","Finally, to determine the significance of the on-source we will compare how likely it is for a peak as large or larger to appear in the background. Our background will be empirically measured by taking portions of the SNR time series from the \"off-source,\" i.e., times that do not overlap the on-source. An important criteria to avoid a biased significance estimate is that the background and experiment be performed in the same manner."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"hjIUBtnuCLJx","scrolled":true},"outputs":[],"source":["from pycbc.detector import Detector\n","\n","# Calculate the time of flight between the Virgo detectors and each LIGO observatory\n","d = Detector(\"V1\")\n","tof = {}\n","tof['H1'] = d.light_travel_time_to_detector(Detector(\"H1\"))\n","tof['L1'] = d.light_travel_time_to_detector(Detector(\"L1\"))\n","\n","# Record the time of the peak in the LIGO observatories\n","ptime = {}\n","\n","pylab.figure(figsize=[14, 4])\n","for ifo in ifos:\n","    \n","    # Shade the region around each LIGO peak that could have a peak in Virgo if from\n","    # an astrophysical source\n","    if ifo != 'V1':\n","        ptime[ifo] = snr[ifo].sample_times[nsnr[ifo].argmax()]\n","        pylab.axvspan(ptime[ifo] - tof[ifo], ptime[ifo] + tof[ifo], alpha=0.2, lw=10)\n","        \n","    pylab.plot(snr[ifo].sample_times, nsnr[ifo], label=ifo)\n","\n","# Calculate the span of time that a Virgo peak could in principle happen in from time of flight\n","# considerations.\n","start = ptime['H1'] - tof['H1']\n","end = ptime['L1'] + tof['L1']\n","\n","# Convert the times to indices along with how large the region is in number of samples\n","window_size = int((end - start) * snr['V1'].sample_rate)\n","sidx = int((start - snr['V1'].start_time) * snr['V1'].sample_rate)\n","eidx = sidx + window_size\n","\n","# Calculate the \"on-source\" peak re-weighted (newsnr) statistic value.\n","onsource = nsnr['V1'][sidx:eidx].max()\n","\n","pylab.legend()\n","pylab.grid()\n","pylab.xlim(m.time - .08, m.time + .08)\n","pylab.ylim(0, 15)\n","pylab.xlabel('Time (s)')\n","pylab.ylabel('Re-weighted Signal-to-noise')\n","pylab.show()\n","\n","print('Virgo Peak has a statistic value of {}'.format(onsource))"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"DHLZIDeFCLJ0"},"source":["In the plot above we see the re-weighted SNR time series. On top of that we shaded the regions which are consistent with a Virgo signal based on the peaks in the LIGO observatories. Only in the darker region it is possible to have a peak in the Virgo SNR timeseries that is consistent with both LIGO observatories."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"D9_fJTDvCLJ0"},"outputs":[],"source":["# Now that we calculated the on-source peak, we should calculate the background peak values.\n","# We do this by chopping up the time series into chunks that are the same size as our\n","# onsource and repeating the same peak finding (max) procedure.\n","\n","# Walk through the data in chunks and calculate the peak statistic value in each.\n","peaks = []\n","i = 0\n","while i + window_size < len(nsnr['V1']):\n","    p = nsnr['V1'][i:i+window_size].max()\n","    peaks.append(p)\n","    i += window_size\n","    \n","    # Skip past the onsource time\n","    if abs(i - sidx) < window_size:\n","        i += window_size * 2\n","    \n","peaks = numpy.array(peaks)"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"Qiym0pdVCLJ3"},"outputs":[],"source":["# The p-value is just the number of samples observed in the background with a \n","# value equal or higher than the onsource divided by the number of samples.\n","# We can make the mapping between statistic value and p-value using our background\n","# samples.\n","pcurve = numpy.arange(1, len(peaks)+1)[::-1] / float(len(peaks))\n","peaks.sort()\n","\n","pvalue = (peaks > onsource).sum() / float(len(peaks))\n","\n","pylab.figure(figsize=[10, 7])\n","pylab.scatter(peaks, pcurve, label='Off-source (Noise Background)', color='black')\n","\n","pylab.axvline(onsource, label='On-source', color='red')\n","pylab.axhline(pvalue, color='red')\n","\n","pylab.legend()\n","pylab.yscale('log')\n","pylab.grid()\n","pylab.ylim(1e-3, 1e0)\n","pylab.ylabel('$p$-value')\n","pylab.xlabel('Re-weighted Signal-to-noise')\n","\n","pylab.xlim(2, 5)\n","pylab.show()\n","\n","print(\"The p-value associated with the GW170814 peak is {}\".format(pvalue))"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"f9w4ucRbCLJ5"},"source":["We find a peak in Virgo as large as the obseved one has an approximately 2% chance of occuring due to the noise alone. Since that is a relatively low probability, we may reject the null hypothesis that the observed peak is due to noise alone. Given the simplifications we carried out, we find a result in agreement with the [GW170814 discovery paper](https://arxiv.org/pdf/1709.09660.pdf) which reported a $p$-value of 0.3%.\n","\n","Had the signal been much louder in the Virgo data, the Virgo peak would be larger than any peak in the noise background. In this case, this method of estimating the significance would only be able to set an upper bound on the $p$-value of the observed peak. In order to calculate the $p$-value of a much larger peak, we would either need to use more background data or make additional assumptions about the background distribution. If a gravitational-wave signal is extremely loud, it is challenging to calculate the precise significance of the observed peak, but we can still be confident that the signal is very significant! "]}]}